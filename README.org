#+STARTUP: overview indent

* Linux TFS development environment
There's a Makefile that will git clone the needed repos.

** Usage Notes
*** Launching a "Host <-> Router <-> Router <-> Host" setup.
You can use the tests to bring up a working iptfs ipsec tunnel. You'll need to
have installed ~qemu~, ~socat~, ~python~ and optional but highly recommended
~tmux~, additionally you'll need to install the python requirements,

#+begin_src bash
  python3 -m venv venv
  source venv/bin/activate
  pip install -r python-requirements.txt
#+end_src

Then run a test adding the `--pause` flag, and it will pause before running the first
test, but after having configured the hosts (h1, h2) and routers (r1, r2).

NOTE: If your `sudo` command forces a restricted PATH (~secure_path~) then the
python virtual environment may not work. In this case `sudo bash` and then
activate the virtual environment as root.

NOTE: SUDO: For best results add the following to your ~/etc/sudoers~ config.
This will allow tmux to continue to work inside the sudo environment.

#+begin_src shell
  Defaults env_keep += "TMUX"
  Defaults env_keep += "TMUX_PANE"
#+end_src

#+begin_src shell
  $ sudo -E pytest -s -v tests/simplenet --pause
  [...]
  == PAUSING: before test 'tests/simplenet/test_simplenet.py::test_net_up' ==
#+end_src

or

#+begin_src shell
  $ sudo -E bash
  # tmux # optional but highly useful
  # source venv/bin/activate
  # pytest -s -v tests/simplenet --pause
  [...]
  == PAUSING: before test 'tests/simplenet/test_simplenet.py::test_net_up' ==
#+end_src


You can now log into the running setup in another terminal. Use ~socat~ to log
into the console of the running qemu'd linux.

#+begin_src shell
  $ sudo socat /dev/stdin,rawer,escape=0x1d,,echo=0,icanon=0 \
      unix-connect:/tmp/unet-test/tests.simplenet.test_simplenet/r1/s/console2
#+end_src

You can use ~mucmd~ to simply enter the namespace of the running node (e.g., to
ping from ~h1~ to ~h2~ over the iptfs tunnel use the following command).

#+begin_src shell
  $ sudo mucmd -d /tmp/unet-test/tests.simplenet.test_simplenet h1 ping 10.0.2.4
  PING 10.0.2.4 (10.0.2.4) 56(84) bytes of data.
  64 bytes from 10.0.2.4: icmp_seq=1 ttl=62 time=1.22 ms
  64 bytes from 10.0.2.4: icmp_seq=2 ttl=62 time=1.40 ms
  64 bytes from 10.0.2.4: icmp_seq=3 ttl=62 time=1.25 ms
  ...
#+end_src

*** Qemu
**** consoles
2 serial consoles are created using unix sockets '/tmp/qemu-sock/console' and
'/tmp/qemu-sock/con2'. These can be accessed in the namespace with the following
command:

~socat /dev/stdin,escape=0x1d,rawer unix-connect:/tmp/qemu-sock/console~

And outside of the namespace with

~socat /dev/stdin,escape=0x1d,rawer unix-connect:<rundir>/<name>/s/console~

Where ~<rundir>~ is usually ~/tmp/unet-root/~, so if the node name is ~r1~:

~sudo socat /dev/stdin,rawer,escape=0x1d,,echo=0,icanon=0 unix-connect:/tmp/unet-root/r1/console~

If your ~socat~ doesn't support ~rawer~ option replace with ~raw,echo=0,icanon=0~.
**** GDB
#+begin_src bash
  $ sudo gdb linux/vmlinux
  (gdb) target remote /tmp/unet-root/r1/s/gdbserver
  ...

  or
  (gdb)
  target remote /tmp/unet-test/tests.simplenet.test_simplenet/r1/s/gdbserver
  target remote /tmp/unet-test/tests.simplenet.test_simplenet/r2/s/gdbserver

  target remote /tmp/unet-test/tests.errors.test_errors/r1/s/gdbserver
  target remote /tmp/unet-test/tests.errors.test_errors/r2/s/gdbserver

  target remote /tmp/unet-test/tests.phynic.test_simplenet/r1/s/gdbserver

  target remote /tmp/unet-test/tests.stress.test_stress/r2/s/gdbserver

  target remote /tmp/unet-test/tests.utpkt.test_utpkt/r1/s/gdbserver

  target remote /tmp/unet-test/tests.verify.test_verify/r1/s/gdbserver

#+end_src
** Design Notes
*** From Steffen's Mail
[...] look at:

net/xfrm/*
net/ipv4/xfrm*
net/ipv4/esp4*
net/ipv6/xfrm*
net/ipv6/esp6*

> Anything else you think might be useful too would be much appreciated of course.

I think TFS should be a new encapsulation mode. We currently have
tunnel, transport and beet mode (and some odd ipv6 modes). Adding
a tfs_tunnel mode to add all the TFS special stuff would be the
way to go at a first glance. The modes are implemented in:

net/xfrm/xfrm_output.c
net/xfrm/xfrm_input.c

** Bugs
- xfrmi_rcv_cb is looking up xfrm_state from our newly created skb from decaping
  iptfs, but it has not xfrm_state so we panic
  - Need to associate the xfrm_state with new skbs too.. is there a refcnt for this?


* Sandbox

** Sample PPS and packet send times for 1500B IP packets
#+begin_src C :includes <stdio.h> :includes <stdint.h>
#include <stdio.h>
#define ENET_OHEAD (14 + 4 + 8 + 12)
#define _1GE_PPS(iptfs_ip_mtu) ((1e9 / 8) / ((iptfs_ip_mtu) + ENET_OHEAD))
#define _10GE_PPS(iptfs_ip_mtu) ((1e10 / 8) / ((iptfs_ip_mtu) + ENET_OHEAD))
#define _40GE_PPS(iptfs_ip_mtu) ((4e10 / 8) / ((iptfs_ip_mtu) + ENET_OHEAD))
#define _100GE_PPS(iptfs_ip_mtu) ((1e11 / 8) / ((iptfs_ip_mtu) + ENET_OHEAD))
#define _1GE_PP_NANOS(iptfs_ip_mtu) (1e9 / _1GE_PPS(iptfs_ip_mtu))
#define _10GE_PP_NANOS(iptfs_ip_mtu) (1e9 / _10GE_PPS(iptfs_ip_mtu))
#define _40GE_PP_NANOS(iptfs_ip_mtu) (1e9 / _40GE_PPS(iptfs_ip_mtu))
#define _100GE_PP_NANOS(iptfs_ip_mtu) (1e9 / _100GE_PPS(iptfs_ip_mtu))

int mtu = 64;

printf("+ 1GE 10GE 40GE 100GE\n");
printf("PPS %lu %lu %lu %lu\n", (uint64_t)_1GE_PPS(mtu),(uint64_t)_10GE_PPS(mtu),(uint64_t)_40GE_PPS(mtu),(uint64_t)_100GE_PPS(mtu));
printf("packet-time %luns %luns %luns %luns\n", (uint64_t)_1GE_PP_NANOS(mtu),(uint64_t)_10GE_PP_NANOS(mtu),(uint64_t)_40GE_PP_NANOS(mtu),(uint64_t)_100GE_PP_NANOS(mtu));
#+end_src

#+RESULTS:
| +           | 1GE     | 10GE     | 40GE     | 100GE     |
| PPS         | 1225490 | 12254901 | 49019607 | 122549019 |
| packet-time | 816ns   | 81ns     | 20ns     | 8ns       |


** Performance Triaging
Testing done on a single server with 3 networks cards using munet and wiring the
ports to each other. Performance is nowhere near line rate even for routed packets.
[h1] - [r1] - [r2] - [h2]
       ===========

*** Qemu emulated - single socket/core
- Routed from h1 to h2 iperf bidir:              ~2000 Mbps
- IPsec [r1,r2] tunnel, from h1 to h2 iperf bidir ~120 Mbps
- IPTFS [r1,r2] tunnel, from h1 to h2 iperf bidir   ~2 Mbps
*** Qemu -accel kvm - single socket/core
- Routed from h1 to h2 iperf bidir:              ~9400 Mbps
- IPsec [r1,r2] tunnel, from h1 to h2 iperf bidir ~920 Mbps
- IPTFS [r1,r2] tunnel, from h1 to h2 iperf bidir   ~2 Mbps
*** Qemu -accel kvm - 4 sockets
- Routed from h1 to h2 iperf bidir:               ~9400 Mbps
- IPsec [r1,r2] tunnel, from h1 to h2 iperf bidir ~7200 Mbps
- IPTFS [r1,r2] tunnel, from h1 to h2 iperf bidir     700Kbps-3.87Mbps
